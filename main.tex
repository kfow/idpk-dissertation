\documentclass{l4proj}
\usepackage{listings}
\usepackage{minted}
\usemintedstyle{emacs}
\newcommand{\code}[1]{\texttt{#1}}
%% Language and font encodings
% \usepackage[english]{babel}
% \usepackage[utf8x]{inputenc}
% \usepackage[T1]{fontenc}

% %% Sets page size and margins
% \usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% %% Useful packages
% \usepackage{amsmath}
% \usepackage{graphicx}
% \usepackage[colorinlistoftodos]{todonotes}
% \usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Public vs. Private: \newline Identifying Public Domain Knowledge}
\author{Kelvin Fowler}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
The current system of sensitivity review used in the archival process of government documents is slow and insecure. Information Retrieval techniques can be leveraged to vastly improve this operation. This project aims to provide a system and an interface, incorporating these information retrieval techniques, to allow archivists to identify information from documents which already exists within the public domain.
Expand this.
\end{abstract}

% \renewcommand{\abstractname}{Acknowledgements}
% \begin{abstract}
% I would like to thank my supervisors Dr Craig Macdonald and Graham McDonald for their continued assistance throughout the course of this project.
% \end{abstract}

\educationalconsent

\tableofcontents

\chapter{Introduction}
\pagenumbering{arabic}

\section{Aims}
The aims of this project.
Help government archivists to identify when a document contains information already in the public domain. Armed with this knowledge, the archivist can make a more informed decision regarding the public availability of a document.

\section{Motivations}
The current system of document review involves no official public domain knowledge checking process. Often, archivists are forced to use regular web search engines, which poses a potential security risk as certain queries regarding document contents are, by their very nature, security sensitive.

\section{Structure}
This paper will discuss the challenges and decisions involved in the design, implementation and evaluation of this system. We will identify the project's relevance through reviewing some related literature. We will take an in-depth look at the planning and design of the system before moving on to explain the key implementation challenges and solutions.
We will finally discuss the evaluation techniques used to improve the system and conclude with learnings and conclusions.

\chapter{Related Literature}
\section{Sensitivity Review}
Towards a Classifier for Digital Sensitivity Review. Graham McDonald, Craig Macdonald, Iadh Ounis, and Timothy Gollins. In Proceedings of the 36th European Conference on Information Retrieval 2014, (ECIR 2014)

On Using Information Retrieval for the Selection and Sensitivity Review of Digital Public Records. Timothy Gollins, Graham McDonald, Craig Macdonald, and Iadh Ounis. In Proceedings of Privacy-Preserving IR: When Information Retrieval Meets Privacy and Security, workshop @ SIGIR 2014, (PIR 2014)
\subsection{Paper Processing}
The current sensitivity review procedures used by government archivists are less then ideal. Papers
\subsection{Assistive Technology for Sensitivity}

\section{Information Retrieval}
\subsection{Search Engines}
Terrier, Lucene
\subsection{Ad-hoc Retrieval}
\subsection{High Recall Tasks}
Precision, Recall, Have you got all of the relevant documents
E-Discovery, Patent Search

Oard, D. W., \& Webber, W. Information retrieval for e-discovery. Foundations and Trends® in Information Retrieval, 7(2–3), 99-237.(2013)
Provides a good introduction to e-discovery
\subsection{Entities}
Wikify!, Other papers

\section{Gap/Need}
M. Moss. Where Have All the Files Gone? Lost in Action Points Every One? J. Contemporary History, 47(4), 2012.
See P867 for some background on the introduction of born digital documents in government and P872 for a bit about 'the gap’ you are addressing.

\chapter{Requirements and Design}
\section{Requirements}
Throughout the project, requirements were identified and categorised using the MoSCoW method. This means that each requirement is identified as either Must Have, Should Have, Could Have or Would Like to Have.
Initially these requirements were elicited from the project supervisors, who had extensive knowledge of the problem domain, through related research and a working relationship with the project stakeholders, archivists in the British Government(?).
These initial requirements allowed the developer to begin work on the project, after which more detailed requirements were identified through discussion and experimentation.

\subsection{Functional Requirements}
The functional requirements of the project were further ratified through user stories. 
TODO: Reference for Virtues of user stories + reference?
\paragraph{Must Have\\}
M1 - Front end user interface displaying source document for review and target documents automatically identified by the software.
      As a document reviewer (archivist)
      I want to view both the document up for review and associated press and Wiki articles
      So that I can easily compare contents of both and see what is known in the public domain.

M2 - Trials of different retrieval methods and some form of quantitative analysis based on these.
      As a document reviewer
      I want to be confident that this software will return the most relevant results in reasonable time
      So that I can trust the service to return excellent related documents.

\paragraph{Should Have}
S1 - Generate suggested queries to allow user to refine returned related documents.
      As a document reviewer
      I want to refine search terms for related documents
      So that more relevant documents are returned

S2 - Ability to run custom queries.
      As a document reviewer
      I want to write my own queries
      So that I can retrieve the best results and be sure that the automatic query system is performing as well as possible.

S3 - Date range searches:
      As a document reviewer
      I want to limit date range for retrieval of related documents
      So that I can ensure relevance of related documents

\paragraph{Could Have}
C1 - Machine learning to improve system performance over time based on usage by reviewers
	As a document reviewer
	I want my actions using the system to improve it’s performance to better suit my needs
	So that it is easier to find related documents that I would deem relevant

C2 - Tf-idf (Term Frequency - Inverse Document Frequency) analysis to better identify relevant documents.
      As a document reviewer
      I want to only have returned to me related documents which have been identified as containing search terms in meaningful ways.
      So that I can be sure the returned documents contain information as relevant as possible to the search terms.

\paragraph{Would Like to Have}
W1 - Wikify! Like functionality to automatically highlight and link to concepts in source documents which have wikipedia articles associated with them.
      As a document reviewer,
      I want to have easy access to wikipedia articles of items mentioned in the document I am reviewing,
      So that I can easily find out more about concepts mentioned in documents.

\subsection{Non-Functional Requirements}
Easy to use user interface. \\
Results must be relevant. \\
Must return results in reasonable time. \\
Scalability to handle new large amounts of files. \\
Present results in an intuitive and understandable way. \\

\section{Design and Architecture}
The product/application is based on a client-server design. More specifically, a web application acts as a client which interfaces with a RESTful API acting as a server.
The client communicates with the server using HTTP requests which request information from the server or update information on the server.
The web application maintains no state between usages, but rather requests all necessary data from the server at upon loading. Having no state system in the client application greatly reduces the complexity of the web application.
State and persistence is managed on the server entirely using Terrier and Trec files. This will be discussed in more detail in the server design section.

\subsection{Server Architecture}
\paragraph{Information Retrieval}
For the information retrieval needs of the project, Terrier was used. Terrier is a open-source search engine with an extensive Java API allowing it to be easily used for development of various IR tasks.
Terrier, being developed at The University of Glasgow, was the best and obvious choice of IR technology for the project. The local expertise meant that problems encountered could be easily resolved or explained, ensuring minimal downtime due to learning.
Other potential choices for this section could have been Apache Lucene or MG4J as recommended in Middleton-Baeza's Comparison of Open Source Search Engines \cite{middletonbaeza}.

\paragraph{Persistence}
During indexing, one can save metadata to be retrieved for each document in the index. This provides an excellent opportunity to save some post-analysis information.
The documents pertinent to the project (both source and target) are stored within the server filesystem as .trec files. There is one document per file.
Importantly, during indexing, terrier uses a provided collection.spec file in order to identify files to index. It then assigns these documents document ids in order of analysis. This order corresponds to the line number of the collection.spec.
Thusly, we can leverage this fact to read the collection.spec and find a document path if we have a doc id.
This allows us to read information directly from document files using the paths in the collection.spec.
Furthermore, we can write to these .trec files, and save within them, information too large to save in the meta-index generated during terrier indexing.

Using all of these techniques we have a fully functional state system capable of retaining all the necessary information needed for the system.
This design entirely eliminates the need for a DBMS (Database Management System) like PostgreSQL and the complicated Java to Database Model mapping that often comes with such designs.
We leverage the capabilities of already present technologies to provide persistent state without wasting extensive resources.

\paragraph{Natural Language Processing}
Stanford Natural Language Processing was used as an NLP framework to allow for greater accuracy in queries. Provided is a Java API which allows text to be analyzed and annotated. More specifically, the named entity identification capability of StandordNLP was used.
Stanford NLP was initially chosen due to its preferable documentation. It also offers the ability to identify specifically which tokenisers to use when processing some text allowing the developer to streamline the NLP process to their specific use-case.

\paragraph{RESTful API}
The entire back-end application is based around Java. This was an obvious choice due to Terriers Java API and the necessity to build upon this to create a viable product.
Jersey is a RESTful API framework for Java that allows the programmer to define endpoints for HTTP requests. TODO: Wording
Jackson handles the conversion of JSON to POJOs and Vice-Versa? TODO: Beef this section out.

\subsection{Client Architecture}
\paragraph{Model View ViewModel}
The architecture of the front end user interface is based around the Model View ViewModel design pattern. Facilitated by the JS library Knockout.js, MVVM allows us to separate the logic associated with the DOM (Domain Object Model) TODO: to where?
We no longer need to apply JQuery or Vanilla Javascript to individual DOM elements through classes and ids, but instead we can apply rules identified in the ViewModel to various elements throughout the DOM. This allows for dependency tracking of JS variables and automatic real time updates as their values change.
This lends itself well to the problem domain of many rapidly changing documents and results sets as the user identifies relevant documents.

\paragraph{Style}
Bootstrap was chosen mainly due to familiarity reasons. It provides myriad customisable components, as well as being extremely well documented. This dramatically simplifies the rapid development of a user interface.

\paragraph{Javascript Libraries}
As mentioned in section ?? Knockout.js facilitates the MVVM pattern. There are various other libraries that allow for similar designs, however familiarity with Knockout's syntax and best practices meant that development was easy and fast.
This was important as the learning curve for the backend technologies required more attention, and so ease of development on the frontend was key.
JQuery is used in the front end to interface with the back end through HTTP requests. JQuery is a dependency of both Knockout and Bootstrap and so utilizing it's already present features was a given.

\paragraph{Web Application Framework}
The web application currently runs on top of an Express.js server running on Node.js. This is not particularly necessary as the web application could, in theory, be served through a static web server such as GitHub Pages. What Express.js does allow for, however, is the use of the EJS template system. EJS allows us to define partial HTML pages and insert them into others. This is useful with in the MVVM design as we can initialize parts of the front end with specific ViewModels using the "With Binding" TODO: See Implementation.

\chapter{Server Implementation}
\section{Dependency Management and Build System}
Maven was used as a dependency management and build system. Maven was chosen due to it's ease of use and clear documentation. It allowed for easy inclusion of all the dependencies needed for the project such as Terrier, StanfordNLP and Jersey. Maven's build system also ensures all unit and integration tests pass before building the system into a runnable jar file. Look into how you actually want to build the system. Can't run in eclipse forever.

\section{Indexing}
\subsection{Target Documents}
The system operates on the assumption that there already exists a target index within it's directory structure.
As such, a separate Java program was written which can index target documents if needs be. This program loads the appropriate terrier.properties and invokes indexing through Terriers API. As Target Indexing uses the same Tokeniser as Source Indexing, we can guarantee that matching terms continue to match after NLP analysis

\subsection{Source Documents}
Source indexing is invoked through the REST api. Again this loads the relevant properties and invokes indexing through the Terrier API

\subsection{Named Entity Tokeniser}
As aforementioned NO YOU HAVENT, we wanted to identify Named Entities within documents to enhance query generation and provide better retrieval. Terrier indexing uses a tokeniser to identify terms to add to the index. This provided a hook-in point for named entity identification as we can indentify if terms are Named Entities before they are passed out of the tokeniser to be added into the index.

\section{Retrieval}
Retrieval is invoked through the REST API. A query is passed in. More, too basic

\section{Properties}
Terrier operates with a collection of properties which change the behaviour of it's various operations. These properties can be set inside a terrier.properties file, however this complicates changing these properties between different operations. These properties can be set programatically and so a static class was made which allows for easy switching of settings to facilitate correct invokation of indexing and retrieval. : Talk about Source vs. Target indexing

\section{Test Collection Generation}
To allow for the offline evaluation identified in the : Insert section API methods were added to allow addition of topics and qrels.

\section{URL Paths : Naming}

\section{Summary}
research questions for eval chapter

\chapter{Client Implementation}
This chapter will deal with the steps taken in the implementation of the web application which acts as the client in the client server paradigm.

\subsection{Tools and Methods}
\paragraph{Dependency Management}
Bower was used for dependency management within the realm of frontend javascript. Bower allows a programmer to define dependencies through a bower.json file. These can then be downloaded through a bower install command straight into a bower components folder. This was used to install JQuery, Bootstrap and Knockout.

\paragraph{IDE}
JetBrains WebStorm was used to develop the client web application. It allows for easier and faster development by allowing one to run a node application through it's interface, as well as other niceties such as intelligent code completion.

\section{ViewModel and Data Binding}

\section{Computed Functions and Observables}
Within the ViewModel, the programmer defines Observables instead of plain JS variables. These ensure inclusion of the logic needed to interact with the DOM through data-binding. Observables can also be "computed". This means that their value depends on an automatically evaluating function. These functions re-evaluate any time an observable which is a dependency to the function changes. This system allows for complex logic which can be fired off automatically depending on certain circumstances.
This feature can also be exploited to allow for automatic firing of events not related to any specific observable. This is very useful in this project's context as we can send off API requests when certain observables change, for example the requested source document.

\section{Components for Tab System}
Knockout provides a component system which allows one to register new html elements, along with a specific view model with which these objects interact. The programmer provides a html template (complete with data-binding) and a ViewModel. When instantiating one of these components one passes in arguments to fill the ViewModel.
This system lends itself excellently to the tab system present in the system. Using a ``foreach" binding along with these custom components a fully functional tab system exists, which is, at it's foundation, simply an array of docNo's. Such is the power of declarative bindings and custom components! Haha!

\section{Summary}
In order to validate user acceptance testing it was decided that variants of the front end system were to be created. These variants would play on some of the main features of the FrontEnd design, in order to identify the UI elements which were helping and hindering the user experience.

\chapter{Evaluation}
\section{Offline Evaluation Using Terrier}
\subsection{Aims and Method}
Offline evaluation was performed using the evaluation tools provided with Terrier. This evaluation allows one to produce detailed statistics regarding the performance of an IR system.\\
For 20 source documents, a number of target documents were reviewed and rated as either relevant or non relevant. This formed the ground truth against which the evaluation could be performed.\\
As discussed above, for each source document, 4 queries were automatically generated. Evaluations were run for each of the 4 query formulations with the aim of discovering which query it would be most suitable to run automatically upon a source document loading.\\
We were most interested in the Mean Average Precision scores generated by the offline evaluation.\\
Mean Average Precision (MAP) is a measure of the effectiveness of an IR system.
It is given by:
\begin{displaymath}
  MAP=\frac{\sum_{n=1}^{Q} Ave(P(q)}{Q}
\end{displaymath}
Where: 
\begin{itemize}
\item{~$Q$ is the number of queries.}
\item{~$Ave(P(q))$ is is the average precision of a given query.}
\end{itemize}

\subsection{Results}

The query which performed the worst was the query formed from the subject line. This can be expected, as other than running Named Entity recognition on the line, no other analysis was done to improve it's performance. There is also no guarantee that the subject line will contain keywords which provide vital context to a query.\\
Take, for example, this subject from a document in the source collection:
\begin{center}
``PARLIAMENT'S FALL SESSION: A PREVIEW"
\end{center}
We are given no insight into which country this refers to and although this is not true for many documents, the results clearly demonstrate the lack of depth in subject queries causes under-performance.\\
The Tf-Idf Named Entities query scores almost double the subject query. It is limited to the top 10 highest scoring named entities in terms of Tf-Idf, meaning the query is highly relevant to the given document. Here terms are weighted more heavily if they appear frequently in the source document, but infrequently in the source collection. This (in theory) generates a more specific, focused query. \\
The query consisting of all named entities identified in the document was second only to the query of all terms. This suggests that the inclusion of verbs and other non named entities produces a more meaningful query.\\
We can see from Table \ref{results} and A GRAPH, that the most highly performing queries consist of more terms and take longer to process. Ultimately, however, the performance increase of the all terms query is too great to allow processing time to take precedence and so, this was the query chosen to run automatically upon a source document loading.\\w
\begin{center}
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Query                 & MAP    & Mean Query Length & Mean Query Processing Time (S) \\ \hline
All Terms             & 0.4549 & 358.5             & 2.66615                        \\ \hline
Named Entities        & 0.3976 & 93.45             & 0.4478                         \\ \hline
Tf-Idf Named Entities & 0.3040 & 10                & 0.22595                        \\ \hline
Subject               & 0.1594 & 9.75              & 0.15835                        \\ \hline
\end{tabular}
\caption{Terrier Evaluation Results}
\label{results}
\end{table}
\end{center}

\section{User Evaluation}
\subsection{Think-Aloud}

NAME-UNKNOWN, a key potential beneficiary of such a system participated in a small think-aloud study while examining the software in use. Some of the key points learned from this session are listed below:

\begin{itemize}
\item \textit{``Named Entity importance is nuanced and frequency (or Tf-Idf) of named entities does not necessarily produce the most important terms.''}
\par
This raises an important point regarding the limitations of automatic query generation.\textbf{There is of course the ability to run custom queries. But this could be improved somewhat}

\item \textit{``Further term highlighting or importance weighting is necessary to help produce the best results.''}
\par
This point highlight the fact that the queries generated right now are fairly general, and with some deep understanding of the way archivists review documents could help produce better results. Briefly mentioned was weighting the first and last paragraphs higher than the middle of the document, as these are likely to contain the most pertinent information.

\item \textit{``More advanced methods could be leveraged to improve yet further the capabilities of the software, like eye tracking or automatic querying upon highlighting''}
\par
Although beyond the scope of this project, this point identifies the potential expansion of this project. Eye-tracking could be used to automatically track what the user is interested in from the source document, forming the basis of new automatic queries. A simplified version of this could use text highlighting to accomplish the same results.

\item \textit{``UI considerations are equally as important as information retrieval considerations when it comes to improving the effectiveness of the software''}
\par
At this point in the project, the implementation of most of the IR functionality was complete. It now became important to consider how best to modify and improve the user interface to ensure the easiest experience possible. - DECORATION
\end{itemize}

\textbf{Talk about how this allowed iteration and improvement}

\section{Refining the System}
\section{Testing}
\subsection{Unit Testing}
JUnit was used to organise and run unit tests on the various classes present in the backend application.
The Arrange-Act-Assert pattern was used to organise unit tests in a structured way. This arrangement allows for clear identification of the method being tested, separate from the code needed to prepare for the test.
Testing necessitated the refactoring of some of the files which read and write information to files. Let us observe an example in the form of the TargetDocument.java class. This class analyses a compressed target document to create a representation which can be converted to JSON and delivered to the Web App for viewing.
The constructor originally took a file path and constructed the necessary streams and readers to allow for reading of the file.
\begin{minted}{java}
public TargetDocument(String path){
  try {
    FileInputStream stream = new FileInputStream(path);
    GZIPInputStream gzStream = new  GZIPInputStream(stream);
    InputStreamReader inputStreamReader = new  InputStreamReader(gzStream);
    BufferedReader br = new  BufferedReader(inputStreamReader);
    documentParser(br);
...
\end{minted}

This approach is inflexible as it assumes the file path will direct to a gzipped file. It means that unit tests need either a sandbox file system to prepare tests, or some kind of mocking of the file system. Mocking a file system is overly complex and adding a sandbox filesystem adds unpredictable side effects.
A better, more general, and self contained approach is to have the constructor take a \code{Reader} interface as an argument. We can then instantiate the necessary \code{BufferedReader} needed.

\begin{minted}{java}
public TargetDocument(Reader r){
    BufferedReader br = new BufferedReader(r);
    this.docNo = this.title = this.date = this.keywords = this.body = "";
    documentParser(br);
...
\end{minted}

This approach was extended to several classes which handle interacting with files.
Code which interacted with the Terrier API posed problems in term of unit testing. Terrier often introduces side effects in the form of interacting with established index files in the file system.

\section{Conclusion}

\chapter{Conclusions}
\section{Fulfilled Requirements}
\section{Possible Improvements}
Machine Learning.

Reflective

%%%%%%%%%%%%%%%%
%              %
%  APPENDICES  %
%              %
%%%%%%%%%%%%%%%%
\begin{appendices}  r

\end{appendices}

%%%%%%%%%%%%%%%%%%%%
%   BIBLIOGRAPHY   %
%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{bib}

\end{document}
